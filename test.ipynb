{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psnr = [18.680141, 14.5373, 16.678677, 16.937946, 18.557934, 14.577075, 17.545328, 18.21395, 18.189692, 18.35741]\n",
    "ssim = [0.47412324, 0.47946468, 0.50667745, 0.5182882, 0.40986508, 0.4980549, 0.63974226, 0.63125074, 0.5714904, 0.4930398]\n",
    "psnr_fine = [18.790533, 14.478284, 16.743114, 16.868551, 18.565668, 14.662598, 17.602137, 18.225819, 18.223934, 18.400364]\n",
    "ssim_fine = [0.47979182, 0.48036927, 0.50494874, 0.5174515, 0.4187762, 0.49905854, 0.6366769, 0.62079793, 0.57648945, 0.49423438]\n",
    "miou = [0.52838165, 0.42462546, 0.36073428, 0.31907237, 0.3830865, 0.2829054, 0.51568216, 0.4454773, 0.5013875, 0.29229742]\n",
    "\n",
    "print(\"psnr: \", sum(psnr)/len(psnr))\n",
    "print(\"ssim: \", sum(ssim)/len(ssim))\n",
    "print(\"psnr_fine: \", sum(psnr_fine)/len(psnr_fine))\n",
    "print(\"ssim_fine: \", sum(ssim_fine)/len(ssim_fine))\n",
    "print(\"miou: \", sum(miou)/len(miou))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scannet time:  0.020211458206176758\n",
      "scannet time:  0.7548654079437256\n",
      "scannet time:  0.5323371887207031\n",
      "train_ids:\n",
      " ['0', '5', '10', '15', '20', '25', '30', '35', '40', '45', '50', '55', '60', '65', '70', '75', '80', '85', '90', '95', '100', '105', '110', '115', '120', '125', '130', '135', '140', '145', '150', '155', '160', '165', '170', '175', '180', '185', '190', '195', '200', '205', '210', '215', '220', '225', '230', '235', '240', '245', '250', '255', '260', '265', '270', '275', '280', '285', '290', '295', '300', '305', '310', '315', '320', '325', '330', '335', '340', '345', '350', '355', '360', '365', '370', '375', '380', '385', '390', '395', '400', '405', '410', '415', '420', '425', '430', '435', '440', '445', '450', '455', '460', '465', '470', '475', '480', '485', '490', '495', '500', '505', '510', '515', '520', '525', '530', '535', '540', '545', '550', '555', '560', '565', '570', '575', '580', '585', '590', '595', '600', '605', '610', '615', '620', '625', '630', '635', '640', '645', '650', '655', '660', '665', '670', '675', '680', '685', '690', '695']\n",
      "val_ids:\n",
      " ['2', '22', '42', '62', '82', '102', '122', '142', '162', '182']\n",
      "scannet_val time:  0.049345970153808594\n",
      "scannet_val time:  0.5875437259674072\n",
      "scannet_val time:  0.5157372951507568\n",
      "klevr time:  0.35121893882751465\n",
      "klevr time:  0.3987276554107666\n"
     ]
    }
   ],
   "source": [
    "from data.klevr import KlevrDataset\n",
    "from data.scannet import RendererDataset\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "\n",
    "begin1 = time.time()\n",
    "scannet_dataset = RendererDataset(\n",
    "    root_dir=\"/mnt/sdb/timothy/Desktop/2023Spring/Semantic-Ray/data/scannet\",\n",
    "    is_train=True\n",
    ")\n",
    "print(\"scannet time: \", time.time() - begin1)\n",
    "begin11 = time.time()\n",
    "scannet_data = scannet_dataset[0]\n",
    "print(\"scannet time: \", time.time() - begin11)\n",
    "begin12 = time.time()\n",
    "scannet_data1 = scannet_dataset[1]\n",
    "print(\"scannet time: \", time.time() - begin12)\n",
    "\n",
    "'''\n",
    "validation dataset need to be modified\n",
    "'''\n",
    "begin1 = time.time()\n",
    "val_scenes = np.loadtxt(\"configs/lists/scannetv2_val_split.txt\", dtype=str).tolist()\n",
    "for name in val_scenes:\n",
    "    val_cfg = {'val_database_name': name}\n",
    "    val_set = RendererDataset(cfg=val_cfg, is_train=False, root_dir=\"/mnt/sdb/timothy/Desktop/2023Spring/Semantic-Ray/data/scannet\")\n",
    "    break\n",
    "print(\"scannet_val time: \", time.time() - begin1)\n",
    "begin11 = time.time()\n",
    "val_scannet_data = val_set[0]\n",
    "print(\"scannet_val time: \", time.time() - begin11)\n",
    "begin12 = time.time()\n",
    "val_scannet_data1 = val_set[1]\n",
    "print(\"scannet_val time: \", time.time() - begin12)\n",
    "\n",
    "begin2 = time.time()\n",
    "dataset = KlevrDataset(\n",
    "    root_dir=\"/mnt/sdb/timothy/Desktop/2023Spring/GeoNeRF/data/data/nesf_data/klevr/\",\n",
    "    split=\"val\",\n",
    "    nb_views=6,\n",
    "    get_semantic=True,\n",
    ")\n",
    "print(\"klevr time: \", time.time() - begin2)\n",
    "begin22 = time.time()\n",
    "data = dataset[0]\n",
    "print(\"klevr time: \", time.time() - begin22)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['affine_mats', 'affine_mats_inv', 'c2ws', 'closest_idxs', 'depths', 'depths_aug', 'depths_h', 'images', 'intrinsics', 'near_fars', 'semantics', 'w2cs']\n",
      "['affine_mats', 'affine_mats_inv', 'c2ws', 'closest_idxs', 'depths', 'depths_aug', 'depths_h', 'images', 'intrinsics', 'near_fars', 'semantics', 'w2cs']\n",
      "['affine_mats', 'affine_mats_inv', 'c2ws', 'closest_idxs', 'depths', 'depths_aug', 'depths_h', 'images', 'intrinsics', 'near_fars', 'semantics', 'w2cs']\n"
     ]
    }
   ],
   "source": [
    "print(sorted(data.keys()))\n",
    "print(sorted(val_scannet_data.keys()))\n",
    "print(sorted(scannet_data.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images nparray (7, 3, 256, 256)\n",
      "semantics nparray (7, 256, 256)\n",
      "w2cs nparray (7, 4, 4)\n",
      "c2ws nparray (7, 4, 4)\n",
      "intrinsics nparray (7, 3, 3)\n",
      "affine_mats nparray (7, 4, 4, 3)\n",
      "affine_mats_inv nparray (7, 4, 4, 3)\n",
      "closest_idxs nparray (6, 5)\n",
      "depths_aug nparray (7, 64, 64)\n",
      "depths_h nparray (7, 256, 256)\n",
      "depths dict\n",
      "level_0 (7, 256, 256)\n",
      "level_1 (7, 128, 128)\n",
      "level_2 (7, 64, 64)\n",
      "near_fars nparray (7, 2)\n"
     ]
    }
   ],
   "source": [
    "for k, v in data.items():\n",
    "    if isinstance(v, torch.Tensor):\n",
    "        print(k, 'tensor', v.shape)\n",
    "    elif isinstance(v, np.ndarray):\n",
    "        print(k, 'nparray', v.shape)\n",
    "    elif isinstance(v, dict):\n",
    "        print(k, 'dict')\n",
    "        for k1, v1 in v.items():\n",
    "            print(k1, v1.shape)\n",
    "    else:\n",
    "        print(k, type(v))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images nparray (9, 3, 240, 320)\n",
      "semantics nparray (9, 240, 320)\n",
      "c2ws nparray (9, 4, 4)\n",
      "intrinsics nparray (9, 3, 3)\n",
      "near_fars nparray (9, 2)\n",
      "depths_h nparray (9, 240, 320)\n",
      "closest_idxs nparray (8, 4)\n",
      "w2cs nparray (9, 4, 4)\n",
      "affine_mats nparray (9, 4, 4, 3)\n",
      "affine_mats_inv nparray (9, 4, 4, 3)\n",
      "depths_aug nparray (9, 60, 80)\n",
      "depths dict\n",
      "level_0 (9, 240, 320)\n",
      "level_1 (9, 120, 160)\n",
      "level_2 (9, 60, 80)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "for k, v in scannet_data.items():\n",
    "    if isinstance(v, torch.Tensor):\n",
    "        print(k, 'tensor', v.shape)\n",
    "    elif isinstance(v, np.ndarray):\n",
    "        print(k, 'nparray', v.shape)\n",
    "    elif isinstance(v, dict):\n",
    "        print(k, 'dict')\n",
    "        for k1, v1 in v.items():\n",
    "            print(k1, v1.shape)\n",
    "    else:\n",
    "        print(k, type(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images nparray (9, 3, 240, 320)\n",
      "semantics nparray (9, 240, 320)\n",
      "c2ws nparray (9, 4, 4)\n",
      "intrinsics nparray (9, 3, 3)\n",
      "near_fars nparray (9, 2)\n",
      "depths_h nparray (9, 240, 320)\n",
      "closest_idxs nparray (8, 4)\n",
      "w2cs nparray (9, 4, 4)\n",
      "affine_mats nparray (9, 4, 4, 3)\n",
      "affine_mats_inv nparray (9, 4, 4, 3)\n",
      "depths_aug nparray (9, 60, 80)\n",
      "depths dict\n",
      "level_0 (9, 240, 320)\n",
      "level_1 (9, 120, 160)\n",
      "level_2 (9, 60, 80)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "for k, v in val_scannet_data.items():\n",
    "    if isinstance(v, torch.Tensor):\n",
    "        print(k, 'tensor', v.shape)\n",
    "    elif isinstance(v, np.ndarray):\n",
    "        print(k, 'nparray', v.shape)\n",
    "    elif isinstance(v, dict):\n",
    "        print(k, 'dict')\n",
    "        for k1, v1 in v.items():\n",
    "            print(k1, v1.shape)\n",
    "    else:\n",
    "        print(k, type(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['semantics'].min(), data['semantics'].max())\n",
    "print(scannet_data['semantics'].min(), scannet_data['semantics'].max())\n",
    "print(val_scannet_data['semantics'].min(), val_scannet_data['semantics'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[\"depths\"][\"level_0\"].shape)\n",
    "print(data[\"depths\"][\"level_1\"].shape)\n",
    "print(data[\"depths\"][\"level_2\"].shape)\n",
    "print(data[\"depths_aug\"].shape)\n",
    "print(data[\"depths_h\"].shape)\n",
    "print(data[\"near_fars\"])\n",
    "print(data[\"intrinsics\"].shape)\n",
    "print(data[\"w2cs\"].shape)\n",
    "print(type(data[\"depths_h\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "source_depths = data[\"depths_h\"][:6]\n",
    "target_depths = data[\"depths_h\"][6:]\n",
    "points = []\n",
    "H, W = 256, 256\n",
    "\n",
    "ys, xs = torch.meshgrid(\n",
    "    torch.linspace(0, H - 1, H), torch.linspace(0, W - 1, W), indexing=\"ij\"\n",
    ")  # pytorch's meshgrid has indexing='ij'\n",
    "for num in range(source_depths.shape[0]):\n",
    "    mask = source_depths[num] > 0\n",
    "    # print(mask.shape)\n",
    "    ys, xs = ys.reshape(-1), xs.reshape(-1)\n",
    "\n",
    "    dirs = torch.stack(\n",
    "    [\n",
    "        (xs - data[\"intrinsics\"][num][0, 2]) / data[\"intrinsics\"][num][0, 0],\n",
    "        (ys - data[\"intrinsics\"][num][1, 2]) / data[\"intrinsics\"][num][1, 1],\n",
    "        torch.ones_like(xs),\n",
    "    ],\n",
    "    -1,\n",
    "    )\n",
    "    rays_dir = (\n",
    "        dirs @ torch.asarray(data[\"c2ws\"][num][:3, :3]).t()\n",
    "    )\n",
    "    rays_orig = torch.asarray(data[\"c2ws\"][num][:3, -1]).clone().reshape(1, 3).expand(rays_dir.shape[0], -1)\n",
    "    rays_orig = rays_orig.reshape(H,W,-1)[mask]\n",
    "    rays_depth = torch.asarray(source_depths[num]).reshape(H,W,-1)[mask]\n",
    "    rays_dir = rays_dir.reshape(H,W,-1)[mask]\n",
    "    print(rays_orig.shape)\n",
    "    print(rays_dir.shape)\n",
    "    print(rays_depth.shape)\n",
    "    ray_pts = rays_orig + rays_depth * rays_dir\n",
    "    points.append(ray_pts.reshape(-1,3))\n",
    "\n",
    "points = torch.cat(points,0).reshape(-1,3)\n",
    "print(points.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From Github https://github.com/balcilar/DenseDepthMap\n",
    "def dense_map(Pts, n, m, grid):\n",
    "    ng = 2 * grid + 1\n",
    "    \n",
    "    mX = torch.zeros((m,n)) + torch.tensor(100000).to(torch.float32)\n",
    "    mY = torch.zeros((m,n)) + torch.tensor(100000).to(torch.float32)\n",
    "    mD = torch.zeros((m,n))\n",
    "    y_ = Pts[1].to(torch.int32)\n",
    "    x_ = Pts[0].to(torch.int32)\n",
    "    mX[y_,x_] = Pts[0] - torch.round(Pts[0])\n",
    "    mY[y_,x_] = Pts[1] - torch.round(Pts[1])\n",
    "    mD[y_,x_] = Pts[2]\n",
    "    \n",
    "    KmX = torch.zeros((ng, ng, m - ng, n - ng))\n",
    "    KmY = torch.zeros((ng, ng, m - ng, n - ng))\n",
    "    KmD = torch.zeros((ng, ng, m - ng, n - ng))\n",
    "    \n",
    "    for i in range(ng):\n",
    "        for j in range(ng):\n",
    "            KmX[i,j] = mX[i : (m - ng + i), j : (n - ng + j)] - grid - 1 +i\n",
    "            KmY[i,j] = mY[i : (m - ng + i), j : (n - ng + j)] - grid - 1 +i\n",
    "            KmD[i,j] = mD[i : (m - ng + i), j : (n - ng + j)]\n",
    "    S = torch.zeros_like(KmD[0,0])\n",
    "    Y = torch.zeros_like(KmD[0,0])\n",
    "    \n",
    "    for i in range(ng):\n",
    "        for j in range(ng):\n",
    "            s = 1/torch.sqrt(KmX[i,j] * KmX[i,j] + KmY[i,j] * KmY[i,j])\n",
    "            Y = Y + s * KmD[i,j]\n",
    "            S = S + s\n",
    "    \n",
    "    S[S == 0] = 1\n",
    "    out = torch.zeros((m,n))\n",
    "    out[grid + 1 : -grid, grid + 1 : -grid] = Y/S\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Calibration:\n",
    "    def __init__(self, calib_filepath):\n",
    "        calibs = self.read_calib_file(calib_filepath)\n",
    "\n",
    "        self.P = calibs['P2']\n",
    "        self.P = np.reshape(self.P, [3,4])\n",
    "\n",
    "        self.W2C = calibs['Tr_velo_to_cam']\n",
    "        self.W2C = np.reshape(self.W2C, [3,4])\n",
    "\n",
    "        self.R0 = calibs['R0_rect']\n",
    "        self.R0 = np.reshape(self.R0,[3,3])\n",
    "\n",
    "    @staticmethod\n",
    "    def read_calib_file(filepath):\n",
    "        data = {}\n",
    "        with open(filepath, 'r') as f:\n",
    "            for line in f.readlines():\n",
    "                line = line.rstrip()\n",
    "                if len(line)==0: continue\n",
    "                key, value = line.split(':', 1)\n",
    "                try:\n",
    "                    data[key] = np.array([float(x) for x in value.split()])\n",
    "                except ValueError:\n",
    "                    pass\n",
    "        return data\n",
    "    \n",
    "    # From LiDAR coordinate system to Camera Coordinate system\n",
    "    def lidar2cam(self, pts_3d_lidar):\n",
    "        n = pts_3d_lidar.shape[0]\n",
    "        pts_3d_hom = np.hstack((pts_3d_lidar, np.ones((n,1))))\n",
    "        pts_3d_cam_ref = np.dot(pts_3d_hom, np.transpose(self.W2C))\n",
    "        # pts_3d_cam_rec = np.transpose(np.dot(self.R0, np.transpose(pts_3d_cam_ref)))\n",
    "        return pts_3d_cam_ref\n",
    "    \n",
    "    # From Camera Coordinate system to Image frame\n",
    "    def rect2Img(self, rect_pts, img_width, img_height):\n",
    "        n = rect_pts.shape[0]\n",
    "        points_hom = np.hstack((rect_pts, np.ones((n,1))))\n",
    "        points_2d = np.dot(points_hom, np.transpose(self.P)) # nx3\n",
    "        points_2d[:,0] /= points_2d[:,2]\n",
    "        points_2d[:,1] /= points_2d[:,2]\n",
    "        \n",
    "        mask = (points_2d[:,0] >= 0) & (points_2d[:,0] <= img_width) & (points_2d[:,1] >= 0) & (points_2d[:,1] <= img_height)\n",
    "        mask = mask & (rect_pts[:,2] > 2)\n",
    "        return points_2d[mask,0:2], mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2c_ref = torch.asarray(data[\"w2cs\"][6])\n",
    "intrinsics_ref = torch.asarray(data[\"intrinsics\"][6])\n",
    "img_width = 256\n",
    "img_height = 256\n",
    "\n",
    "R = w2c_ref[:3, :3]  # (3, 3)\n",
    "T = w2c_ref[:3, 3:]  # (3, 1)\n",
    "ray_pts = torch.matmul(points, R.t()) + T.reshape(1, 3)\n",
    "\n",
    "ray_pts_ndc = ray_pts @ intrinsics_ref.t()\n",
    "ray_pts_ndc[:, 0] /= ray_pts_ndc[:, 2]\n",
    "ray_pts_ndc[:, 1] /= ray_pts_ndc[:, 2]\n",
    "mask = (ray_pts_ndc[:, 0] >= 0) & (ray_pts_ndc[:, 0] <= img_width) & (ray_pts_ndc[:, 1] >= 0) & (ray_pts_ndc[:, 1] <= img_height)\n",
    "mask = mask & (ray_pts[:, 2] > 2)\n",
    "points_2d = ray_pts_ndc[mask, 0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lidarOnImage = torch.cat((points_2d, ray_pts[mask,2].reshape(-1,1)), 1)\n",
    "\n",
    "out = dense_map(lidarOnImage.T, 256, 256, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(out.shape)\n",
    "print(out[125:130,125:130])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(points.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch3d\n",
    "print(pytorch3d.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from pytorch3d.renderer import (\n",
    "    FoVPerspectiveCameras, look_at_view_transform,\n",
    "    RasterizationSettings, BlendParams,\n",
    "    MeshRenderer, MeshRasterizer, HardPhongShader, PointsRasterizationSettings, PointsRasterizer\n",
    ")\n",
    "point_cloud = pytorch3d.structures.pointclouds.Pointclouds(points.unsqueeze(0))\n",
    "class PointsRenderer(nn.Module):\n",
    "    def __init__(self, rasterizer, compositor) -> None:\n",
    "        super().__init__()\n",
    "        self.rasterizer = rasterizer\n",
    "        self.compositor = compositor\n",
    "\n",
    "    def forward(self, point_clouds, **kwargs) -> torch.Tensor:\n",
    "        fragments = self.rasterizer(point_clouds, **kwargs)\n",
    "\n",
    "        r = self.rasterizer.raster_settings.radius\n",
    "\n",
    "        dists2 = fragments.dists.permute(0, 3, 1, 2)\n",
    "        weights = 1 - dists2 / (r * r)\n",
    "        images = self.compositor(\n",
    "            fragments.idx.long().permute(0, 3, 1, 2),\n",
    "            weights,\n",
    "            point_clouds.features_packed().permute(1, 0),\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        # permute so image comes at the end\n",
    "        images = images.permute(0, 2, 3, 1)\n",
    "\n",
    "        return images, fragments.zbuf\n",
    "    \n",
    "device = torch.device(\"cuda\")\n",
    "R = torch.asarray(data[\"c2ws\"][6:,:3,:3]).to(\"cuda\")\n",
    "T = torch.asarray(data[\"c2ws\"][6:,:3,-1]).to(\"cuda\")\n",
    "camera = FoVPerspectiveCameras(device=device, R=R, T=T)\n",
    "\n",
    "raster_settings = PointsRasterizationSettings(\n",
    "        image_size=(256, 256),\n",
    "        radius=0.007,\n",
    "        points_per_pixel=15,\n",
    "    )\n",
    "\n",
    "rasterizer = PointsRasterizer(\n",
    "    cameras=camera, \n",
    "    raster_settings=raster_settings\n",
    ")\n",
    "renderer = PointsRenderer(rasterizer=rasterizer, compositor=pytorch3d.renderer.points.compositor.AlphaCompositor())\n",
    "print(point_cloud.features_packed())\n",
    "images, fragments = renderer(point_cloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cameras = # camera object derived from CamerasBase\n",
    "xyz = # 3D points of shape (batch_size, num_points, 3)\n",
    "# transform xyz to the camera view coordinates\n",
    "xyz_cam = cameras.get_world_to_view_transform().transform_points(xyz)\n",
    "# extract the depth of each point as the 3rd coord of xyz_cam\n",
    "depth = xyz_cam[:, :, 2:]\n",
    "# project the points xyz to the camera\n",
    "xy = cameras.transform_points(xyz)[:, :, :2]\n",
    "# append depth to xy\n",
    "xy_depth = torch.cat((xy, depth), dim=2)\n",
    "# unproject to the world coordinates\n",
    "xyz_unproj_world = cameras.unproject_points(xy_depth, world_coordinates=True)\n",
    "print(torch.allclose(xyz, xyz_unproj_world)) # True\n",
    "# unproject to the camera coordinates\n",
    "xyz_unproj = cameras.unproject_points(xy_depth, world_coordinates=False)\n",
    "print(torch.allclose(xyz_cam, xyz_unproj)) # True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.utils import visualize_depth\n",
    "\n",
    "depth_minmax = [\n",
    "                0.9 * scannet_data[\"near_fars\"].min(),\n",
    "                1.1 * scannet_data[\"near_fars\"].max(),\n",
    "            ]\n",
    "# plt.figure()\n",
    "\n",
    "#subplot(r,c) provide the no. of rows and columns\n",
    "f, axarr = plt.subplots(1,scannet_data[\"depths_h\"].shape[0]) \n",
    "\n",
    "# use the created array to output your multiple images. In this case I have stacked 4 images vertically\n",
    "for i in range(scannet_data[\"depths_h\"].shape[0]):\n",
    "    novel_view_depth_color = visualize_depth(scannet_data[\"depths_h\"][0], depth_minmax)[0]\n",
    "    depth_vis = novel_view_depth_color.permute(1,2,0).numpy()\n",
    "    axarr[i].imshow(depth_vis)\n",
    "\n",
    "\n",
    "# plt.imshow(depth_vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kornia\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from utils.utils import visualize_depth\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "depth = Image.open(\"/home/timothy/Desktop/2023Spring/generalized_nerf/logs_klevr/klevr/0531_architecture_memory_corrected_5/evaluation_/00000000/target_depth_00.png\")\n",
    "# depth = plt.imread(\"/home/timothy/Desktop/2023Spring/generalized_nerf/logs_klevr/klevr/0531_architecture_memory_corrected_5/evaluation_/00000000/target_depth_00.png\")\n",
    "depth = torch.from_numpy(np.array(depth)).to(torch.uint8).to(torch.float32).permute(2,0,1)\n",
    "print(np.asarray(depth_)[0,0])\n",
    "print(depth[:,0,0])\n",
    "\n",
    "print(depth.max())\n",
    "print(depth.shape)\n",
    "print(depth.dtype)\n",
    "filtered_depth = kornia.filters.bilateral_blur(depth.unsqueeze(0), (5, 5), 1, (2, 2))\n",
    "print(filtered_depth.max())\n",
    "print(filtered_depth.shape)\n",
    "print(filtered_depth.dtype)\n",
    "# depth = visualize_depth(depth, [0.1, 17.])[0]\n",
    "# filtered_depth = visualize_depth(filtered_depth, [0.1, 17.])[0]\n",
    "depth = (depth.permute(1,2,0)*255).cpu().numpy().astype(np.uint8)\n",
    "filtered_depth = (filtered_depth.squeeze(0).permute(1,2,0)*255).cpu().numpy().astype(np.uint8)\n",
    "print(depth.shape)\n",
    "print(filtered_depth.shape)\n",
    "print(depth.max())\n",
    "print(filtered_depth.max())\n",
    "f, axarr = plt.subplots(1,2)\n",
    "axarr[0].imshow(depth)\n",
    "axarr[1].imshow(filtered_depth)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1,2,3,4,5,6,7,8,9,10])\n",
    "print(x.shape)\n",
    "print(x.unsqueeze(0).unsqueeze(0).shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0bbd8b2400cb2f722383fac4897cde6fdb10a3a63e2c81520acb32adb5a87cfb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
