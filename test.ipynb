{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "psnr:  17.227545300000003\n",
      "ssim:  0.5221996750000001\n",
      "psnr_fine:  17.2561002\n",
      "ssim_fine:  0.5228594729999999\n",
      "miou:  0.405365004\n"
     ]
    }
   ],
   "source": [
    "psnr = [18.680141, 14.5373, 16.678677, 16.937946, 18.557934, 14.577075, 17.545328, 18.21395, 18.189692, 18.35741]\n",
    "ssim = [0.47412324, 0.47946468, 0.50667745, 0.5182882, 0.40986508, 0.4980549, 0.63974226, 0.63125074, 0.5714904, 0.4930398]\n",
    "psnr_fine = [18.790533, 14.478284, 16.743114, 16.868551, 18.565668, 14.662598, 17.602137, 18.225819, 18.223934, 18.400364]\n",
    "ssim_fine = [0.47979182, 0.48036927, 0.50494874, 0.5174515, 0.4187762, 0.49905854, 0.6366769, 0.62079793, 0.57648945, 0.49423438]\n",
    "miou = [0.52838165, 0.42462546, 0.36073428, 0.31907237, 0.3830865, 0.2829054, 0.51568216, 0.4454773, 0.5013875, 0.29229742]\n",
    "\n",
    "print(\"psnr: \", sum(psnr)/len(psnr))\n",
    "print(\"ssim: \", sum(ssim)/len(ssim))\n",
    "print(\"psnr_fine: \", sum(psnr_fine)/len(psnr_fine))\n",
    "print(\"ssim_fine: \", sum(ssim_fine)/len(ssim_fine))\n",
    "print(\"miou: \", sum(miou)/len(miou))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scannet time:  0.00035071372985839844\n",
      "scannet time:  1.4460539817810059\n",
      "scannet time:  1.8903958797454834\n",
      "train_ids:\n",
      " ['0', '5', '10', '15', '20', '25', '30', '35', '40', '45', '50', '55', '60', '65', '70', '75', '80', '85', '90', '95', '100', '105', '110', '115', '120', '125', '130', '135', '140', '145', '150', '155', '160', '165', '170', '175', '180', '185', '190', '195', '200', '205', '210', '215', '220', '225', '230', '235', '240', '245', '250', '255', '260', '265', '270', '275', '280', '285', '290', '295', '300', '305', '310', '315', '320', '325', '330', '335', '340', '345', '350', '355', '360', '365', '370', '375', '380', '385', '390', '395', '400', '405', '410', '415', '420', '425', '430', '435', '440', '445', '450', '455', '460', '465', '470', '475', '480', '485', '490', '495', '500', '505', '510', '515', '520', '525', '530', '535', '540', '545', '550', '555', '560', '565', '570', '575', '580', '585', '590', '595', '600', '605', '610', '615', '620', '625', '630', '635', '640', '645', '650', '655', '660', '665', '670', '675', '680', '685', '690', '695']\n",
      "val_ids:\n",
      " ['2', '22', '42', '62', '82', '102', '122', '142', '162', '182']\n",
      "scannet_val time:  0.18852853775024414\n",
      "scannet_val time:  0.6345350742340088\n",
      "scannet_val time:  0.7404708862304688\n",
      "klevr time:  0.0938262939453125\n",
      "klevr time:  0.49040913581848145\n"
     ]
    }
   ],
   "source": [
    "from data.klevr import KlevrDataset\n",
    "from data.scannet import RendererDataset\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "\n",
    "begin1 = time.time()\n",
    "scannet_dataset = RendererDataset(\n",
    "    root_dir=\"/home/timothy/Desktop/2023Spring/Semantic-Ray/data/scannet\",\n",
    "    is_train=True\n",
    ")\n",
    "print(\"scannet time: \", time.time() - begin1)\n",
    "begin11 = time.time()\n",
    "scannet_data = scannet_dataset[0]\n",
    "print(\"scannet time: \", time.time() - begin11)\n",
    "begin12 = time.time()\n",
    "scannet_data1 = scannet_dataset[1]\n",
    "print(\"scannet time: \", time.time() - begin12)\n",
    "\n",
    "'''\n",
    "validation dataset need to be modified\n",
    "'''\n",
    "begin1 = time.time()\n",
    "val_scenes = np.loadtxt(\"configs/lists/scannetv2_val_split.txt\", dtype=str).tolist()\n",
    "for name in val_scenes:\n",
    "    val_cfg = {'val_database_name': name}\n",
    "    val_set = RendererDataset(cfg=val_cfg, is_train=False, root_dir=\"/home/timothy/Desktop/2023Spring/Semantic-Ray/data/scannet\")\n",
    "    break\n",
    "print(\"scannet_val time: \", time.time() - begin1)\n",
    "begin11 = time.time()\n",
    "val_scannet_data = val_set[0]\n",
    "print(\"scannet_val time: \", time.time() - begin11)\n",
    "begin12 = time.time()\n",
    "val_scannet_data1 = val_set[1]\n",
    "print(\"scannet_val time: \", time.time() - begin12)\n",
    "\n",
    "begin2 = time.time()\n",
    "dataset = KlevrDataset(\n",
    "    root_dir=\"/home/timothy/Desktop/2023Spring/GeoNeRF/data/data/nesf_data/klevr/\",\n",
    "    split=\"val\",\n",
    "    nb_views=6,\n",
    "    get_semantic=True,\n",
    ")\n",
    "print(\"klevr time: \", time.time() - begin2)\n",
    "begin22 = time.time()\n",
    "data = dataset[0]\n",
    "print(\"klevr time: \", time.time() - begin22)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['affine_mats', 'affine_mats_inv', 'c2ws', 'closest_idxs', 'depths', 'depths_aug', 'depths_h', 'images', 'intrinsics', 'near_fars', 'semantics', 'w2cs']\n",
      "['affine_mats', 'affine_mats_inv', 'c2ws', 'closest_idxs', 'depths', 'depths_aug', 'depths_h', 'images', 'intrinsics', 'near_fars', 'semantics', 'w2cs']\n",
      "['affine_mats', 'affine_mats_inv', 'c2ws', 'closest_idxs', 'depths', 'depths_aug', 'depths_h', 'images', 'intrinsics', 'near_fars', 'semantics', 'w2cs']\n"
     ]
    }
   ],
   "source": [
    "print(sorted(data.keys()))\n",
    "print(sorted(val_scannet_data.keys()))\n",
    "print(sorted(scannet_data.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images nparray (7, 3, 256, 256)\n",
      "semantics nparray (7, 256, 256)\n",
      "w2cs nparray (7, 4, 4)\n",
      "c2ws nparray (7, 4, 4)\n",
      "intrinsics nparray (7, 3, 3)\n",
      "affine_mats nparray (7, 4, 4, 3)\n",
      "affine_mats_inv nparray (7, 4, 4, 3)\n",
      "closest_idxs nparray (6, 5)\n",
      "depths_aug nparray (7, 64, 64)\n",
      "depths_h nparray (7, 256, 256)\n",
      "depths dict\n",
      "level_0 (7, 256, 256)\n",
      "level_1 (7, 128, 128)\n",
      "level_2 (7, 64, 64)\n",
      "near_fars nparray (7, 2)\n"
     ]
    }
   ],
   "source": [
    "for k, v in data.items():\n",
    "    if isinstance(v, torch.Tensor):\n",
    "        print(k, 'tensor', v.shape)\n",
    "    elif isinstance(v, np.ndarray):\n",
    "        print(k, 'nparray', v.shape)\n",
    "    elif isinstance(v, dict):\n",
    "        print(k, 'dict')\n",
    "        for k1, v1 in v.items():\n",
    "            print(k1, v1.shape)\n",
    "    else:\n",
    "        print(k, type(v))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images tensor torch.Size([9, 3, 240, 320])\n",
      "semantics tensor torch.Size([9, 240, 320])\n",
      "w2cs tensor torch.Size([9, 4, 4])\n",
      "intrinsics tensor torch.Size([9, 3, 3])\n",
      "near_fars tensor torch.Size([9, 2])\n",
      "depths_h tensor torch.Size([9, 240, 320])\n",
      "closest_idxs tensor torch.Size([8, 4])\n",
      "c2ws tensor torch.Size([9, 4, 4])\n",
      "affine_mats nparray (9, 4, 4, 3)\n",
      "affine_mats_inv nparray (9, 4, 4, 3)\n",
      "depths_aug nparray (9, 60, 80)\n",
      "depths dict\n",
      "level_0 (9, 240, 320)\n",
      "level_1 (9, 120, 160)\n",
      "level_2 (9, 60, 80)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "for k, v in scannet_data.items():\n",
    "    if isinstance(v, torch.Tensor):\n",
    "        print(k, 'tensor', v.shape)\n",
    "    elif isinstance(v, np.ndarray):\n",
    "        print(k, 'nparray', v.shape)\n",
    "    elif isinstance(v, dict):\n",
    "        print(k, 'dict')\n",
    "        for k1, v1 in v.items():\n",
    "            print(k1, v1.shape)\n",
    "    else:\n",
    "        print(k, type(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images tensor torch.Size([9, 3, 240, 320])\n",
      "semantics tensor torch.Size([9, 240, 320])\n",
      "w2cs tensor torch.Size([9, 4, 4])\n",
      "intrinsics tensor torch.Size([9, 3, 3])\n",
      "near_fars tensor torch.Size([9, 2])\n",
      "depths_h tensor torch.Size([9, 240, 320])\n",
      "closest_idxs tensor torch.Size([8, 4])\n",
      "c2ws tensor torch.Size([9, 4, 4])\n",
      "affine_mats nparray (9, 4, 4, 3)\n",
      "affine_mats_inv nparray (9, 4, 4, 3)\n",
      "depths_aug nparray (9, 60, 80)\n",
      "depths dict\n",
      "level_0 (9, 240, 320)\n",
      "level_1 (9, 120, 160)\n",
      "level_2 (9, 60, 80)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "for k, v in val_scannet_data.items():\n",
    "    if isinstance(v, torch.Tensor):\n",
    "        print(k, 'tensor', v.shape)\n",
    "    elif isinstance(v, np.ndarray):\n",
    "        print(k, 'nparray', v.shape)\n",
    "    elif isinstance(v, dict):\n",
    "        print(k, 'dict')\n",
    "        for k1, v1 in v.items():\n",
    "            print(k1, v1.shape)\n",
    "    else:\n",
    "        print(k, type(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1 <span style=\"color: #00ffff; text-decoration-color: #00ffff\">print</span>(data[<span style=\"color: #808000; text-decoration-color: #808000\">'semantic'</span>].min(), data[<span style=\"color: #808000; text-decoration-color: #808000\">'semantic'</span>].max())                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2 </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">print</span>(scannet_data[<span style=\"color: #808000; text-decoration-color: #808000\">'semantic'</span>].min(), scannet_data[<span style=\"color: #808000; text-decoration-color: #808000\">'semantic'</span>].max())                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3 </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">print</span>(val_scannet_data[<span style=\"color: #808000; text-decoration-color: #808000\">'semantic'</span>].min(), val_scannet_data[<span style=\"color: #808000; text-decoration-color: #808000\">'semantic'</span>].max())                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">4 </span>                                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">KeyError: </span><span style=\"color: #008000; text-decoration-color: #008000\">'semantic'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m1\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1 \u001b[96mprint\u001b[0m(data[\u001b[33m'\u001b[0m\u001b[33msemantic\u001b[0m\u001b[33m'\u001b[0m].min(), data[\u001b[33m'\u001b[0m\u001b[33msemantic\u001b[0m\u001b[33m'\u001b[0m].max())                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2 \u001b[0m\u001b[96mprint\u001b[0m(scannet_data[\u001b[33m'\u001b[0m\u001b[33msemantic\u001b[0m\u001b[33m'\u001b[0m].min(), scannet_data[\u001b[33m'\u001b[0m\u001b[33msemantic\u001b[0m\u001b[33m'\u001b[0m].max())                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m3 \u001b[0m\u001b[96mprint\u001b[0m(val_scannet_data[\u001b[33m'\u001b[0m\u001b[33msemantic\u001b[0m\u001b[33m'\u001b[0m].min(), val_scannet_data[\u001b[33m'\u001b[0m\u001b[33msemantic\u001b[0m\u001b[33m'\u001b[0m].max())                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m4 \u001b[0m                                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mKeyError: \u001b[0m\u001b[32m'semantic'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(data['semantics'].min(), data['semantics'].max())\n",
    "print(scannet_data['semantics'].min(), scannet_data['semantics'].max())\n",
    "print(val_scannet_data['semantics'].min(), val_scannet_data['semantics'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[\"depths\"][\"level_0\"].shape)\n",
    "print(data[\"depths\"][\"level_1\"].shape)\n",
    "print(data[\"depths\"][\"level_2\"].shape)\n",
    "print(data[\"depths_aug\"].shape)\n",
    "print(data[\"depths_h\"].shape)\n",
    "print(data[\"near_fars\"])\n",
    "print(data[\"intrinsics\"].shape)\n",
    "print(data[\"w2cs\"].shape)\n",
    "print(type(data[\"depths_h\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "source_depths = data[\"depths_h\"][:6]\n",
    "target_depths = data[\"depths_h\"][6:]\n",
    "points = []\n",
    "H, W = 256, 256\n",
    "\n",
    "ys, xs = torch.meshgrid(\n",
    "    torch.linspace(0, H - 1, H), torch.linspace(0, W - 1, W), indexing=\"ij\"\n",
    ")  # pytorch's meshgrid has indexing='ij'\n",
    "for num in range(source_depths.shape[0]):\n",
    "    mask = source_depths[num] > 0\n",
    "    # print(mask.shape)\n",
    "    ys, xs = ys.reshape(-1), xs.reshape(-1)\n",
    "\n",
    "    dirs = torch.stack(\n",
    "    [\n",
    "        (xs - data[\"intrinsics\"][num][0, 2]) / data[\"intrinsics\"][num][0, 0],\n",
    "        (ys - data[\"intrinsics\"][num][1, 2]) / data[\"intrinsics\"][num][1, 1],\n",
    "        torch.ones_like(xs),\n",
    "    ],\n",
    "    -1,\n",
    "    )\n",
    "    rays_dir = (\n",
    "        dirs @ torch.asarray(data[\"c2ws\"][num][:3, :3]).t()\n",
    "    )\n",
    "    rays_orig = torch.asarray(data[\"c2ws\"][num][:3, -1]).clone().reshape(1, 3).expand(rays_dir.shape[0], -1)\n",
    "    rays_orig = rays_orig.reshape(H,W,-1)[mask]\n",
    "    rays_depth = torch.asarray(source_depths[num]).reshape(H,W,-1)[mask]\n",
    "    rays_dir = rays_dir.reshape(H,W,-1)[mask]\n",
    "    print(rays_orig.shape)\n",
    "    print(rays_dir.shape)\n",
    "    print(rays_depth.shape)\n",
    "    ray_pts = rays_orig + rays_depth * rays_dir\n",
    "    points.append(ray_pts.reshape(-1,3))\n",
    "\n",
    "points = torch.cat(points,0).reshape(-1,3)\n",
    "print(points.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From Github https://github.com/balcilar/DenseDepthMap\n",
    "def dense_map(Pts, n, m, grid):\n",
    "    ng = 2 * grid + 1\n",
    "    \n",
    "    mX = torch.zeros((m,n)) + torch.tensor(100000).to(torch.float32)\n",
    "    mY = torch.zeros((m,n)) + torch.tensor(100000).to(torch.float32)\n",
    "    mD = torch.zeros((m,n))\n",
    "    y_ = Pts[1].to(torch.int32)\n",
    "    x_ = Pts[0].to(torch.int32)\n",
    "    mX[y_,x_] = Pts[0] - torch.round(Pts[0])\n",
    "    mY[y_,x_] = Pts[1] - torch.round(Pts[1])\n",
    "    mD[y_,x_] = Pts[2]\n",
    "    \n",
    "    KmX = torch.zeros((ng, ng, m - ng, n - ng))\n",
    "    KmY = torch.zeros((ng, ng, m - ng, n - ng))\n",
    "    KmD = torch.zeros((ng, ng, m - ng, n - ng))\n",
    "    \n",
    "    for i in range(ng):\n",
    "        for j in range(ng):\n",
    "            KmX[i,j] = mX[i : (m - ng + i), j : (n - ng + j)] - grid - 1 +i\n",
    "            KmY[i,j] = mY[i : (m - ng + i), j : (n - ng + j)] - grid - 1 +i\n",
    "            KmD[i,j] = mD[i : (m - ng + i), j : (n - ng + j)]\n",
    "    S = torch.zeros_like(KmD[0,0])\n",
    "    Y = torch.zeros_like(KmD[0,0])\n",
    "    \n",
    "    for i in range(ng):\n",
    "        for j in range(ng):\n",
    "            s = 1/torch.sqrt(KmX[i,j] * KmX[i,j] + KmY[i,j] * KmY[i,j])\n",
    "            Y = Y + s * KmD[i,j]\n",
    "            S = S + s\n",
    "    \n",
    "    S[S == 0] = 1\n",
    "    out = torch.zeros((m,n))\n",
    "    out[grid + 1 : -grid, grid + 1 : -grid] = Y/S\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Calibration:\n",
    "    def __init__(self, calib_filepath):\n",
    "        calibs = self.read_calib_file(calib_filepath)\n",
    "\n",
    "        self.P = calibs['P2']\n",
    "        self.P = np.reshape(self.P, [3,4])\n",
    "\n",
    "        self.W2C = calibs['Tr_velo_to_cam']\n",
    "        self.W2C = np.reshape(self.W2C, [3,4])\n",
    "\n",
    "        self.R0 = calibs['R0_rect']\n",
    "        self.R0 = np.reshape(self.R0,[3,3])\n",
    "\n",
    "    @staticmethod\n",
    "    def read_calib_file(filepath):\n",
    "        data = {}\n",
    "        with open(filepath, 'r') as f:\n",
    "            for line in f.readlines():\n",
    "                line = line.rstrip()\n",
    "                if len(line)==0: continue\n",
    "                key, value = line.split(':', 1)\n",
    "                try:\n",
    "                    data[key] = np.array([float(x) for x in value.split()])\n",
    "                except ValueError:\n",
    "                    pass\n",
    "        return data\n",
    "    \n",
    "    # From LiDAR coordinate system to Camera Coordinate system\n",
    "    def lidar2cam(self, pts_3d_lidar):\n",
    "        n = pts_3d_lidar.shape[0]\n",
    "        pts_3d_hom = np.hstack((pts_3d_lidar, np.ones((n,1))))\n",
    "        pts_3d_cam_ref = np.dot(pts_3d_hom, np.transpose(self.W2C))\n",
    "        # pts_3d_cam_rec = np.transpose(np.dot(self.R0, np.transpose(pts_3d_cam_ref)))\n",
    "        return pts_3d_cam_ref\n",
    "    \n",
    "    # From Camera Coordinate system to Image frame\n",
    "    def rect2Img(self, rect_pts, img_width, img_height):\n",
    "        n = rect_pts.shape[0]\n",
    "        points_hom = np.hstack((rect_pts, np.ones((n,1))))\n",
    "        points_2d = np.dot(points_hom, np.transpose(self.P)) # nx3\n",
    "        points_2d[:,0] /= points_2d[:,2]\n",
    "        points_2d[:,1] /= points_2d[:,2]\n",
    "        \n",
    "        mask = (points_2d[:,0] >= 0) & (points_2d[:,0] <= img_width) & (points_2d[:,1] >= 0) & (points_2d[:,1] <= img_height)\n",
    "        mask = mask & (rect_pts[:,2] > 2)\n",
    "        return points_2d[mask,0:2], mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2c_ref = torch.asarray(data[\"w2cs\"][6])\n",
    "intrinsics_ref = torch.asarray(data[\"intrinsics\"][6])\n",
    "img_width = 256\n",
    "img_height = 256\n",
    "\n",
    "R = w2c_ref[:3, :3]  # (3, 3)\n",
    "T = w2c_ref[:3, 3:]  # (3, 1)\n",
    "ray_pts = torch.matmul(points, R.t()) + T.reshape(1, 3)\n",
    "\n",
    "ray_pts_ndc = ray_pts @ intrinsics_ref.t()\n",
    "ray_pts_ndc[:, 0] /= ray_pts_ndc[:, 2]\n",
    "ray_pts_ndc[:, 1] /= ray_pts_ndc[:, 2]\n",
    "mask = (ray_pts_ndc[:, 0] >= 0) & (ray_pts_ndc[:, 0] <= img_width) & (ray_pts_ndc[:, 1] >= 0) & (ray_pts_ndc[:, 1] <= img_height)\n",
    "mask = mask & (ray_pts[:, 2] > 2)\n",
    "points_2d = ray_pts_ndc[mask, 0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lidarOnImage = torch.cat((points_2d, ray_pts[mask,2].reshape(-1,1)), 1)\n",
    "\n",
    "out = dense_map(lidarOnImage.T, 256, 256, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(out.shape)\n",
    "print(out[125:130,125:130])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(points.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch3d\n",
    "print(pytorch3d.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from pytorch3d.renderer import (\n",
    "    FoVPerspectiveCameras, look_at_view_transform,\n",
    "    RasterizationSettings, BlendParams,\n",
    "    MeshRenderer, MeshRasterizer, HardPhongShader, PointsRasterizationSettings, PointsRasterizer\n",
    ")\n",
    "point_cloud = pytorch3d.structures.pointclouds.Pointclouds(points.unsqueeze(0))\n",
    "class PointsRenderer(nn.Module):\n",
    "    def __init__(self, rasterizer, compositor) -> None:\n",
    "        super().__init__()\n",
    "        self.rasterizer = rasterizer\n",
    "        self.compositor = compositor\n",
    "\n",
    "    def forward(self, point_clouds, **kwargs) -> torch.Tensor:\n",
    "        fragments = self.rasterizer(point_clouds, **kwargs)\n",
    "\n",
    "        r = self.rasterizer.raster_settings.radius\n",
    "\n",
    "        dists2 = fragments.dists.permute(0, 3, 1, 2)\n",
    "        weights = 1 - dists2 / (r * r)\n",
    "        images = self.compositor(\n",
    "            fragments.idx.long().permute(0, 3, 1, 2),\n",
    "            weights,\n",
    "            point_clouds.features_packed().permute(1, 0),\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        # permute so image comes at the end\n",
    "        images = images.permute(0, 2, 3, 1)\n",
    "\n",
    "        return images, fragments.zbuf\n",
    "    \n",
    "device = torch.device(\"cuda\")\n",
    "R = torch.asarray(data[\"c2ws\"][6:,:3,:3]).to(\"cuda\")\n",
    "T = torch.asarray(data[\"c2ws\"][6:,:3,-1]).to(\"cuda\")\n",
    "camera = FoVPerspectiveCameras(device=device, R=R, T=T)\n",
    "\n",
    "raster_settings = PointsRasterizationSettings(\n",
    "        image_size=(256, 256),\n",
    "        radius=0.007,\n",
    "        points_per_pixel=15,\n",
    "    )\n",
    "\n",
    "rasterizer = PointsRasterizer(\n",
    "    cameras=camera, \n",
    "    raster_settings=raster_settings\n",
    ")\n",
    "renderer = PointsRenderer(rasterizer=rasterizer, compositor=pytorch3d.renderer.points.compositor.AlphaCompositor())\n",
    "print(point_cloud.features_packed())\n",
    "images, fragments = renderer(point_cloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cameras = # camera object derived from CamerasBase\n",
    "xyz = # 3D points of shape (batch_size, num_points, 3)\n",
    "# transform xyz to the camera view coordinates\n",
    "xyz_cam = cameras.get_world_to_view_transform().transform_points(xyz)\n",
    "# extract the depth of each point as the 3rd coord of xyz_cam\n",
    "depth = xyz_cam[:, :, 2:]\n",
    "# project the points xyz to the camera\n",
    "xy = cameras.transform_points(xyz)[:, :, :2]\n",
    "# append depth to xy\n",
    "xy_depth = torch.cat((xy, depth), dim=2)\n",
    "# unproject to the world coordinates\n",
    "xyz_unproj_world = cameras.unproject_points(xy_depth, world_coordinates=True)\n",
    "print(torch.allclose(xyz, xyz_unproj_world)) # True\n",
    "# unproject to the camera coordinates\n",
    "xyz_unproj = cameras.unproject_points(xy_depth, world_coordinates=False)\n",
    "print(torch.allclose(xyz_cam, xyz_unproj)) # True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_523209/3950368012.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  novel_view_depth_color = visualize_depth(torch.tensor(scannet_data[\"depths_h\"][0]), depth_minmax)[0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">16</span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">13 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">14 # use the created array to output your multiple images. In this case I have stacked 4 im</span>    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">15 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> i <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">range</span>(scannet_data[<span style=\"color: #808000; text-decoration-color: #808000\">\"depths_h\"</span>].shape[<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>]):                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>16 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>novel_view_depth_color = visualize_depth(torch.tensor(scannet_data[<span style=\"color: #808000; text-decoration-color: #808000\">\"depths_h\"</span>][<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>]),     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">17 │   </span>depth_vis = novel_view_depth_color.permute(<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>,<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2</span>,<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>).numpy()                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">18 │   </span>axarr[i].imshow(depth_vis)                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">19 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/timothy/Desktop/2023Spring/generalized_nerf/utils/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">utils.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">215</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">visualize_depth</span>          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">212 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">213 │   │   </span>mi, ma = minmax                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">214 │   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>215 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>x = (x - mi) / (ma - mi + <span style=\"color: #0000ff; text-decoration-color: #0000ff\">1e-8</span>)  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># normalize to 0~1</span>                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">216 │   </span>x = (<span style=\"color: #0000ff; text-decoration-color: #0000ff\">255</span> * x).astype(np.uint8)                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">217 │   </span>x_ = Image.fromarray(cv2.applyColorMap(x, cmap))                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">218 │   </span>x_ = T.ToTensor()(x_)  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># (3, H, W)</span>                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">TypeError: </span>unsupported operand <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">type</span><span style=\"font-weight: bold\">(</span>s<span style=\"font-weight: bold\">)</span> for -: <span style=\"color: #008000; text-decoration-color: #008000\">'numpy.ndarray'</span> and <span style=\"color: #008000; text-decoration-color: #008000\">'Tensor'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m16\u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m13 \u001b[0m                                                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m14 \u001b[0m\u001b[2m# use the created array to output your multiple images. In this case I have stacked 4 im\u001b[0m    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m15 \u001b[0m\u001b[94mfor\u001b[0m i \u001b[95min\u001b[0m \u001b[96mrange\u001b[0m(scannet_data[\u001b[33m\"\u001b[0m\u001b[33mdepths_h\u001b[0m\u001b[33m\"\u001b[0m].shape[\u001b[94m0\u001b[0m]):                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m16 \u001b[2m│   \u001b[0mnovel_view_depth_color = visualize_depth(torch.tensor(scannet_data[\u001b[33m\"\u001b[0m\u001b[33mdepths_h\u001b[0m\u001b[33m\"\u001b[0m][\u001b[94m0\u001b[0m]),     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m17 \u001b[0m\u001b[2m│   \u001b[0mdepth_vis = novel_view_depth_color.permute(\u001b[94m1\u001b[0m,\u001b[94m2\u001b[0m,\u001b[94m0\u001b[0m).numpy()                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m18 \u001b[0m\u001b[2m│   \u001b[0maxarr[i].imshow(depth_vis)                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m19 \u001b[0m                                                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/timothy/Desktop/2023Spring/generalized_nerf/utils/\u001b[0m\u001b[1;33mutils.py\u001b[0m:\u001b[94m215\u001b[0m in \u001b[92mvisualize_depth\u001b[0m          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m212 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94melse\u001b[0m:                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m213 \u001b[0m\u001b[2m│   │   \u001b[0mmi, ma = minmax                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m214 \u001b[0m\u001b[2m│   \u001b[0m                                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m215 \u001b[2m│   \u001b[0mx = (x - mi) / (ma - mi + \u001b[94m1e-8\u001b[0m)  \u001b[2m# normalize to 0~1\u001b[0m                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m216 \u001b[0m\u001b[2m│   \u001b[0mx = (\u001b[94m255\u001b[0m * x).astype(np.uint8)                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m217 \u001b[0m\u001b[2m│   \u001b[0mx_ = Image.fromarray(cv2.applyColorMap(x, cmap))                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m218 \u001b[0m\u001b[2m│   \u001b[0mx_ = T.ToTensor()(x_)  \u001b[2m# (3, H, W)\u001b[0m                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mTypeError: \u001b[0munsupported operand \u001b[1;35mtype\u001b[0m\u001b[1m(\u001b[0ms\u001b[1m)\u001b[0m for -: \u001b[32m'numpy.ndarray'\u001b[0m and \u001b[32m'Tensor'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e25d7e72670044b38448b2806e6757e4",
       "version_major": 2,
       "version_minor": 0
      },
      "image/png": "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",
      "text/html": [
       "\n",
       "            <div style=\"display: inline-block;\">\n",
       "                <div class=\"jupyter-widgets widget-label\" style=\"text-align: center;\">\n",
       "                    Figure\n",
       "                </div>\n",
       "                <img src='data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAoAAAAHgCAYAAAA10dzkAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAg2klEQVR4nO3dcWzc513H8W/i9s6d1NgboU5ckkUtG0UbZFoqR946QpFFxEbH/kCbhpRZE7SgGYktoiNFjAAbdZk2VCnyQCp0maDgwUQmRKOOYlpGG4eItJGmZAx1DTTqZo+I9ty1xFmchz8qblxtt+f2Z/vueV4vyX/kl9/tnrd0Z316sWFDSikFAADF2LjeBwAAYG0ZgAAAhTEAAQAKYwACABTGAAQAKIwBCABQGAMQAKAwBiAAQGEMQACAwhiAAACFMQABAApjAAIAFMYABAAojAEIAFAYAxAAoDAGIABAYQxAAIDCGIAAAIUxAAEACmMAAgAUxgAEACiMAQgAUBgDEACgMAYgAEBhDEAAgMIYgAAAhTEAAQAKYwACABTGAAQAKIwBCABQGAMQAKAwBiAAQGEMQACAwhiAAACFMQABAApjAAIAFMYABAAojAEIAFAYAxAAoDAGIABAYQxAAIDCGIAAAIUxAAEACmMAAgAUxgAEACiMAQgAUBgDEACgMAYgAEBhDEAAgMIYgAAAhTEAAQAKYwACABQmmwH41a9+NW655ZYYHByMDRs2xJe//OVXfMzDDz8cb3/726Ner8cP//APx+HDh1f9nK+WvsX0dY7c+yLyb9S3mD5yls0AfP7552Pnzp0xMTHR1v1nz56N97znPXHzzTfHqVOn4qMf/Wj80i/9UnzlK19Z5ZO+Ovpa6essufdF5N+or5U+spcyFBHpyJEjL3vPxz/+8fSWt7yl5doHPvCBtHfv3lU8WTX06etkufellH+jPn3kL5tPAFdqeno6RkZGWq7t3bs3pqen1+lE1dLX3fR1v9wb9XW33Pt4ZVes9wHWy8zMTAwMDLRcGxgYiLm5ufif//mfuOqqqxY9Zn5+Pubn55t/vnz5cvz3f/93/MAP/EBs2LBh1c/8/73wwgsxNze37N9/61vfiptvvrnlnquvvjrm5uZidnY2ent747nnnovBwcHYuPHF/w7Qt3b0rbwvIp/GmZmZuHTpUrZ9pb5G9XWPlNKS32OKst4fQa6GaOPj7ze96U3pzjvvbLl2//33p4hIL7zwwpKPOXjwYIqIrL7OnTunr4u/SurLsTH3vpc26uu+r5L6SrMhpZQiMxs2bIgjR47E+973vmXv+Ymf+Il4+9vfHnfffXfz2uc///n46Ec/Go1GY8nHvPS/fhqNRmzfvj3OnTsXmzZtqur4r6ivry/uu++++Nmf/dll7/mZn/mZ2LlzZ9x1113Na3/+538ed9xxR5w7dy7m5uZi27Zt8eyzz0ZfX19E6Fsr+l5dX0Q+jadPn866r9TXqL7usdz3mJIU+0/Aw8PDcfTo0ZZrDz74YAwPDy/7mHq9HvV6fdH1TZs2rfmL/3Wve93LPudNN90UR48ebbnnkUceieHh4ZZr//9je31rR9/K+yLya8y9L6Ks16i+7tNt/3RdqfX+CLIqzz33XHr88cfT448/niIi/eEf/mF6/PHH03/+53+mlFI6cOBA2rdvX/P+J598Mr3uda9Lt99+e/r617+eJiYmUk9PT3rggQfafs5Go5EiIjUajcp7XqrqvnbOrq86+qrvW8l9VaiyMfe+ds+urzq591Wtm89elWwG4EMPPbTkv++Pjo6mlFIaHR1Ne/bsWfSYt73tbalWq6Xrrrsuff7zn1/Rc67lC6jqvk57c+vTt9K+ldxXhSobc+9r9+z6qpN7X9W6+exVyfJnANfK3Nxc9PX1RaPR6LqPv9s5u77OpW9l93Wa3PsivEbbvadT6ctfob/7DABQLgMQAKAwBiAAQGEMQACAwhiAAACFMQABAApjAAIAFMYABAAojAEIAFAYAxAAoDAGIABAYQxAAIDCGIAAAIUxAAEACmMAAgAUxgAEACiMAQgAUBgDEACgMAYgAEBhDEAAgMIYgAAAhTEAAQAKYwACABTGAAQAKIwBCABQGAMQAKAwBiAAQGEMQACAwhiAAACFMQABAApjAAIAFMYABAAojAEIAFAYAxAAoDAGIABAYQxAAIDCGIAAAIUxAAEACmMAAgAUxgAEACiMAQgAUBgDEACgMAYgAEBhDEAAgMIYgAAAhTEAAQAKYwACABTGAAQAKIwBCABQGAMQAKAwBiAAQGEMQACAwhiAAACFMQABAApjAAIAFMYABAAojAEIAFAYAxAAoDAGIABAYQxAAIDCGIAAAIUxAAEACmMAAgAUJqsBODExETt27Ije3t7YvXt3nDhx4mXvv/vuu+NHfuRH4qqrropt27bFxz72sbhw4cIanXbl9LXqtr6I/Bv1tdLXWfS16rY+KpYyMTk5mWq1Wrr33nvT6dOn06233pr6+/vT7Ozskvffd999qV6vp/vuuy+dPXs2feUrX0lbt25NH/vYx9p+zkajkSIiNRqNqjKWVXVfO2fXV60qG/W9qFtfo7n3tXt2fdXJva9q3Xz2qmQzAIeGhtLY2FjzzwsLC2lwcDCNj48vef/Y2Fj6qZ/6qZZr+/fvT+985zvbfs61fAFV3ddpb+7c+1KqtlHfi7r1NZp7X0qd9x7U16rb+qrWzWevShb/BHzx4sU4efJkjIyMNK9t3LgxRkZGYnp6esnHvOMd74iTJ082PyJ/8skn4+jRo/Hud797Tc68EvoW66a+iPwb9S2mr3PoW6yb+lgdV6z3Aapw/vz5WFhYiIGBgZbrAwMD8W//9m9LPuYXfuEX4vz583HTTTdFSikuXboUv/IrvxK/+Zu/uezzzM/Px/z8fPPPc3Nz1QS8An2LdVNfRPWNS509976IfF6jufctR9/qyL2P1ZHFJ4CvxsMPPxx33nlnfO5zn4vHHnss/uZv/ibuv//++OQnP7nsY8bHx6Ovr6/5tW3btjU88croW6yb+iJW3ph7X0R3NepbTF/nyL2PNqzbPz5XaH5+PvX09KQjR460XP/Qhz6U3vve9y75mJtuuin9+q//esu1P/uzP0tXXXVVWlhYWPIxFy5cSI1Go/l17ty5NfkZgtXoW+rnH/Stnqobn3nmmeL6UsrnNZp7X6e9B/Ut1k19q8HPAGbyM4C1Wi127doVU1NTzWuXL1+OqampGB4eXvIxL7zwQmzc2Jrf09MTEREppSUfU6/XY9OmTS1fa0HfYt3UF7E2jbn3RXiNrhZ9i+lbbD2/x7AK1m16VmxycjLV6/V0+PDhdObMmXTbbbel/v7+NDMzk1JKad++fenAgQPN+w8ePJiuvvrq9Jd/+ZfpySefTH//93+frr/++vT+97+/7edc61/xr7Kv037DK/e+lKpt1Peibn2N5t7X7tn1VSf3vqp189mrks0ATCmlQ4cOpe3bt6darZaGhobS8ePHm3+3Z8+eNDo62vzz9773vfQ7v/M76frrr0+9vb1p27Zt6SMf+Uh65pln2n6+tX4BVdnXiW/u3PtSqq5R34u69TWae1+7Z9dXrdz7qtTNZ6/KhpSW+ayXVzQ3Nxd9fX3RaDS67qPwds6ur3PpW9l9nSb3vgiv0Xbv6VT68pfFzwACANA+AxAAoDAGIABAYQxAAIDCGIAAAIUxAAEACmMAAgAUxgAEACiMAQgAUBgDEACgMAYgAEBhDEAAgMIYgAAAhTEAAQAKYwACABTGAAQAKIwBCABQGAMQAKAwBiAAQGEMQACAwhiAAACFMQABAApjAAIAFMYABAAojAEIAFAYAxAAoDAGIABAYQxAAIDCGIAAAIUxAAEACmMAAgAUxgAEACiMAQgAUBgDEACgMAYgAEBhDEAAgMIYgAAAhTEAAQAKYwACABTGAAQAKIwBCABQGAMQAKAwBiAAQGEMQACAwhiAAACFMQABAApjAAIAFMYABAAojAEIAFAYAxAAoDAGIABAYQxAAIDCGIAAAIUxAAEACmMAAgAUxgAEACiMAQgAUBgDEACgMAYgAEBhDEAAgMIYgAAAhTEAAQAKYwACABQmqwE4MTERO3bsiN7e3ti9e3ecOHHiZe9/9tlnY2xsLLZu3Rr1ej3e/OY3x9GjR9fotCunr1W39UXk36ivlb7Ooq9Vt/VRsZSJycnJVKvV0r333ptOnz6dbr311tTf359mZ2eXvH9+fj7deOON6d3vfnd65JFH0tmzZ9PDDz+cTp061fZzNhqNFBGp0WhUlbGsqvvaObu+alXZqO9F3foazb2v3bPrq07ufVXr5rNXJZsBODQ0lMbGxpp/XlhYSIODg2l8fHzJ+//oj/4oXXfddenixYuv+jnX8gVUdV+nvblz70up2kZ9K7uvCvpaddt7UF+rbuurWjefvSpZ/BPwxYsX4+TJkzEyMtK8tnHjxhgZGYnp6eklH/O3f/u3MTw8HGNjYzEwMBBvfetb484774yFhYVln2d+fj7m5uZavtaCvsW6qS9ibRpz74vwGl0t+hbTt9h6fo+helkMwPPnz8fCwkIMDAy0XB8YGIiZmZklH/Pkk0/Gl770pVhYWIijR4/GJz7xifjsZz8bn/rUp5Z9nvHx8ejr62t+bdu2rdKO5ehbrJv6ItamMfe+CK/R1aJvMX2Lref3GFbBen8EWYWnn346RUQ6duxYy/Xbb789DQ0NLfmYN73pTWnbtm3p0qVLzWuf/exn05YtW5Z9ngsXLqRGo9H8Onfu3Jp8hLwafUt9/K1v9VTdWGJfSvm8RnPvS6mz3oP6FuumvtXgn4BTumKNduaq2rx5c/T09MTs7GzL9dnZ2diyZcuSj9m6dWtceeWV0dPT07z2oz/6ozEzMxMXL16MWq226DH1ej3q9Xq1h2/DavQtRd/qWYvG3PsivEZXi77F9C22nt9jqF4W/wRcq9Vi165dMTU11bx2+fLlmJqaiuHh4SUf8853vjOeeOKJuHz5cvPav//7v8fWrVuXHH/rSd9i3dQXkX+jvsX0dQ59i3VTH6tkvT+CrMrk5GSq1+vp8OHD6cyZM+m2225L/f39aWZmJqWU0r59+9KBAwea9z/11FPp6quvTr/6q7+avvGNb6S/+7u/S9dcc0361Kc+1fZzrvWv+FfZ12m/4ZV7X0rVNup7Ube+RnPva/fs+qqTe1/VuvnsVclmAKaU0qFDh9L27dtTrVZLQ0ND6fjx482/27NnTxodHW25/9ixY2n37t2pXq+n6667Lv3+7/9+y89DvJK1fgFV2deJb+7c+1KqrlHfi7r1NZp7X7tn11et3Puq1M1nr8qGlFJa3c8Y8zU3Nxd9fX3RaDRi06ZN632cFWnn7Po6l76V3ddpcu+L8Bpt955OpS9/WfwMIAAA7TMAAQAKYwACABTGAAQAKIwBCABQGAMQAKAwBiAAQGEMQACAwhiAAACFMQABAApjAAIAFMYABAAojAEIAFAYAxAAoDAGIABAYQxAAIDCGIAAAIUxAAEACmMAAgAUxgAEACiMAQgAUBgDEACgMAYgAEBhDEAAgMIYgAAAhTEAAQAKYwACABTGAAQAKIwBCABQGAMQAKAwBiAAQGEMQACAwhiAAACFMQABAApjAAIAFMYABAAojAEIAFAYAxAAoDAGIABAYQxAAIDCGIAAAIUxAAEACmMAAgAUxgAEACiMAQgAUBgDEACgMAYgAEBhDEAAgMIYgAAAhTEAAQAKYwACABTGAAQAKIwBCABQGAMQAKAwBiAAQGEMQACAwhiAAACFMQABAApjAAIAFMYABAAojAEIAFAYAxAAoDAGIABAYbIagBMTE7Fjx47o7e2N3bt3x4kTJ9p63OTkZGzYsCHe9773re4BX6Pc+yLyb9S3NH2dQd/S9JGllInJyclUq9XSvffem06fPp1uvfXW1N/fn2ZnZ1/2cWfPnk3XXnttete73pV+7ud+bkXP2Wg0UkSkRqPxGk7enqr72jn7WvalVG2jvvz7VnJfFfQtrVu+x+hbWrf0Va2bz16VbAbg0NBQGhsba/55YWEhDQ4OpvHx8WUfc+nSpfSOd7wj/cmf/EkaHR3t6AFYdV8nvrmrbNSXf99K7quCvsW66XuMvsW6qa9q3Xz2qmTxT8AXL16MkydPxsjISPPaxo0bY2RkJKanp5d93O/93u/FNddcE7/4i7/Y1vPMz8/H3Nxcy9dayL0vYm0a9a2e3F+j+pamr5U+ukkWA/D8+fOxsLAQAwMDLdcHBgZiZmZmycc88sgj8ad/+qdxzz33tP084+Pj0dfX1/zatm3bazp3u3Lvi1ibRn2rJ/fXqL7F9C2mj26SxQBcqeeeey727dsX99xzT2zevLntx91xxx3RaDSaX+fOnVvFU756ufdFvLpGfZ0j99eovqXp6wy599GeK9b7AFXYvHlz9PT0xOzsbMv12dnZ2LJly6L7v/nNb8Z//Md/xC233NK8dvny5YiIuOKKK+Ib3/hGXH/99YseV6/Xo16vV3z6V7YafT/4gz+46HHr1RdRfeO//uu/LnqMvtWzFn0R+bwHc+/rtO8x+lp1Wx+rI4tPAGu1WuzatSumpqaa1y5fvhxTU1MxPDy86P4bbrghvva1r8WpU6eaX+9973vj5ptvjlOnTnXcx9q590VU3/hDP/RDa3n8V6Svlb68+zrte4y+Vt3Wx+rI4hPAiIj9+/fH6Oho3HjjjTE0NBR33313PP/88/HhD384IiI+9KEPxbXXXhvj4+PR29sbb33rW1se39/fHxGx6HqnqLrvwoULa3r+dlTZ2Ik/nKxPX0QZfRGd9z1GX3f3Ub1sBuAHPvCB+K//+q/47d/+7ZiZmYm3ve1t8cADDzR/KPapp56KjRu79wPP3Psi8m/Up6+T6dNHWTaklNJ6H6Jbzc3NRV9fXzQajdi0adN6H2dF2jm7vs6lb2X3dZrc+yK8Rtu9p1Ppy5//HAAAKIwBCABQGAMQAKAwBiAAQGEMQACAwhiAAACFMQABAApjAAIAFMYABAAojAEIAFAYAxAAoDAGIABAYQxAAIDCGIAAAIUxAAEACmMAAgAUxgAEACiMAQgAUBgDEACgMAYgAEBhDEAAgMIYgAAAhTEAAQAKYwACABTGAAQAKIwBCABQGAMQAKAwBiAAQGEMQACAwhiAAACFMQABAApjAAIAFMYABAAojAEIAFAYAxAAoDAGIABAYQxAAIDCGIAAAIUxAAEACmMAAgAUxgAEACiMAQgAUBgDEACgMAYgAEBhDEAAgMIYgAAAhTEAAQAKYwACABTGAAQAKIwBCABQGAMQAKAwBiAAQGEMQACAwhiAAACFMQABAApjAAIAFMYABAAojAEIAFAYAxAAoDAGIABAYQxAAIDCGIAAAIUxAAEACpPVAJyYmIgdO3ZEb29v7N69O06cOLHsvffcc0+8613vite//vXx+te/PkZGRl72/k6Qe19E/o36vk9f59H3ffrIXTYD8Itf/GLs378/Dh48GI899ljs3Lkz9u7dG9/5zneWvP/hhx+OD37wg/HQQw/F9PR0bNu2LX76p386nn766TU+eXty74vIv1FfK32dRV8rfWQvZWJoaCiNjY01/7ywsJAGBwfT+Ph4W4+/dOlSuvrqq9MXvvCFtp+z0WikiEiNRmPF512pqvvaOfta9qVUbaO+/PtS6t73YO59KXXe9xh9L6/T+6rWzWevShafAF68eDFOnjwZIyMjzWsbN26MkZGRmJ6ebut/44UXXojvfe978YY3vGHZe+bn52Nubq7lay3k3hexNo36Vk/ur1F9r0yfPrpLFgPw/PnzsbCwEAMDAy3XBwYGYmZmpq3/jd/4jd+IwcHBljfQS42Pj0dfX1/za9u2ba/p3O3KvS9ibRr1rZ7cX6P6Xpk+fXSXLAbga3XXXXfF5ORkHDlyJHp7e5e974477ohGo9H8Onfu3Bqe8tXLvS+ivUZ9nSv316i+F+nrTLn3sbQr1vsAVdi8eXP09PTE7Oxsy/XZ2dnYsmXLyz72M5/5TNx1113xD//wD/HjP/7jL3tvvV6Per3+ms+7Urn3RaxNo77Vk/trVN/y9H2fPrpJFp8A1mq12LVrV0xNTTWvXb58OaampmJ4eHjZx33605+OT37yk/HAAw/EjTfeuBZHfVVy74vIv1Hf0vR1Bn1L00fW1vu3UKoyOTmZ6vV6Onz4cDpz5ky67bbbUn9/f5qZmUkppbRv37504MCB5v133XVXqtVq6Utf+lL69re/3fx67rnn2n7Otfwtoqr7OvE3vKps1Jd/X0rd+x7Mva/ds+urTu59Vevms1clmwGYUkqHDh1K27dvT7VaLQ0NDaXjx483/27Pnj1pdHS0+ec3vvGNKSIWfR08eLDt51vrF1CVfZ365q6qUV/+fSl173sw9752z66vWrn3Vambz16VDSmltOKPDYmIiLm5uejr64tGoxGbNm1a7+OsSDtn19e59K3svk6Te1+E12i793QqffnL4mcAAQBonwEIAFAYAxAAoDAGIABAYQxAAIDCGIAAAIUxAAEACmMAAgAUxgAEACiMAQgAUBgDEACgMAYgAEBhDEAAgMIYgAAAhTEAAQAKYwACABTGAAQAKIwBCABQGAMQAKAwBiAAQGEMQACAwhiAAACFMQABAApjAAIAFMYABAAojAEIAFAYAxAAoDAGIABAYQxAAIDCGIAAAIUxAAEACmMAAgAUxgAEACiMAQgAUBgDEACgMAYgAEBhDEAAgMIYgAAAhTEAAQAKYwACABTGAAQAKIwBCABQGAMQAKAwBiAAQGEMQACAwhiAAACFMQABAApjAAIAFMYABAAojAEIAFAYAxAAoDAGIABAYQxAAIDCGIAAAIUxAAEACmMAAgAUxgAEACiMAQgAUBgDEACgMAYgAEBhDEAAgMIYgAAAhTEAAQAKk9UAnJiYiB07dkRvb2/s3r07Tpw48bL3//Vf/3XccMMN0dvbGz/2Yz8WR48eXaOTvjq590Xk36ivlb7Ooq+VPrKWMjE5OZlqtVq699570+nTp9Ott96a+vv70+zs7JL3P/roo6mnpyd9+tOfTmfOnEm/9Vu/la688sr0ta99re3nbDQaKSJSo9GoKmNZVfe1c/a17Eup2kZ9+fel1L3vwdz72j27vurk3le1bj57VbIZgENDQ2lsbKz554WFhTQ4OJjGx8eXvP/9739/es973tNybffu3emXf/mX237OtXwBVd3XiW/uKhv15d+XUve+B3PvS6nzvsfoa9VtfVXr5rNX5YrV/XxxbVy8eDFOnjwZd9xxR/Paxo0bY2RkJKanp5d8zPT0dOzfv7/l2t69e+PLX/7yss8zPz8f8/PzzT83Go2IiJibm3sNp39l/9f3a7/2ay3PtWfPnvjnf/7n+MhHPrLoMceOHYuxsbGW+3/yJ38y7r///pibm2teTyk1/369+iJWpzFCX0Q+fRF5vQcj8u7rpNeovu7uWw3LvQeLsm7Ts0JPP/10ioh07Nixluu33357GhoaWvIxV155ZfqLv/iLlmsTExPpmmuuWfZ5Dh48mCIiq69vfvOb+rr4q6S+HBtz73tpo77u+yqprzQbUur++futb30rrr322jh27FgMDw83r3/84x+Pf/qnf4p/+Zd/WfSYWq0WX/jCF+KDH/xg89rnPve5+N3f/d2YnZ1d8nle+l8/zz77bLzxjW+Mp556Kvr6+iosavXtb387brjhhnjwwQdjaGioef0Tn/hEPProo/GP//iPix6zefPm+OM//uP4+Z//+ea1e+65J/7gD/4gnnjiiWg0GrF9+/Z45plnor+/f137IqpvPHnypL7M+iLyeQ/m3tdp32P0dXffaliqrzRZ/BPw5s2bo6enZ9Fwm52djS1btiz5mC1btqzo/oiIer0e9Xp90fW+vr7YtGnTqzh5e3p7e6Onpye++93vtjzPs88+G9dee+2Sz71ly5aYm5tr+bu5ubnYunVry7WNG7//i+Dr1RdRfeP/fTPSl09fRD7vwdz7Ou17jL7u7ltNL30PliSL8lqtFrt27YqpqanmtcuXL8fU1FTLJ4L/3/DwcMv9EREPPvjgsvevp9z7IvJv1LeYvs6hbzF9ZG+9/w26KpOTk6ler6fDhw+nM2fOpNtuuy319/enmZmZlFJK+/btSwcOHGje/+ijj6YrrrgifeYzn0lf//rX08GDBzv+/wxMlX2d+BteVTbqy78vpe59D+be1+7Z9VUn976qdfPZq5LNAEwppUOHDqXt27enWq2WhoaG0vHjx5t/t2fPnjQ6Otpy/1/91V+lN7/5zalWq6W3vOUt6f7771/R8124cCEdPHgwXbhwoYrjv6Iq+9o5+1r3pVRdo778+1ZyX1X0jbbc323fY/SNttzfbX1V6uazVyWLXwIBAKB9WfwMIAAA7TMAAQAKYwACABTGAAQAKIwB+CpNTEzEjh07ore3N3bv3h0nTpxY7yO15atf/WrccsstMTg4GBs2bHjZ/9/H3dio7/v0daZ2G/V1ptxfo/rKYQC+Cl/84hdj//79cfDgwXjsscdi586dsXfv3vjOd76z3kd7Rc8//3zs3LkzJiYmXva+bm3U9yJ9naudRn2dK/fXqL6CrPf/HZpuNDQ0lMbGxpp/XlhYSIODg2l8fHwdT7VyEZGOHDmy5N/l0KhPX6dbrlFfd8j9NVpyXwl8ArhCFy9ejJMnT8bIyEjz2saNG2NkZCSmp6fX8WTVyb1RX3fT191y74vIvzH3vlIYgCt0/vz5WFhYiIGBgZbrAwMDMTMzs06nqlbujfq6m77ulntfRP6NufeVwgAEACiMAbhCmzdvjp6enpidnW25Pjs7G1u2bFmnU1Ur90Z93U1fd8u9LyL/xtz7SmEArlCtVotdu3bF1NRU89rly5djamoqhoeH1/Fk1cm9UV9309fdcu+LyL8x975SXLHeB+hG+/fvj9HR0bjxxhtjaGgo7r777nj++efjwx/+8Hof7RV997vfjSeeeKL557Nnz8apU6fiDW94Q2zfvr15vVsb9b1IX+dqp1Ff58r9NaqvIOv9a8jd6tChQ2n79u2pVquloaGhdPz48fU+UlseeuihFBGLvkZHRxfd242N+r5PX2dqt1FfZ8r9NaqvHBtSSmlVliUAAB3JzwACABTGAAQAKIwBCABQGAMQAKAwBiAAQGEMQACAwhiAAACFMQABAApjAAIAFMYABAAojAEIAFAYAxAAoDAGIABAYQxAAIDCGIAAAIUxAAEACmMAAgAUxgAEACiMAQgAUBgDEACgMAYgAEBhDEAAgMIYgAAAhTEAAQAKYwACABTGAAQAKIwBCABQGAMQAKAwBiAAQGEMQACAwhiAAACF+V9c6gu7QH9XHgAAAABJRU5ErkJggg==' width=640.0/>\n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib widget\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.utils import visualize_depth\n",
    "\n",
    "depth_minmax = [\n",
    "                0.9 * scannet_data[\"near_fars\"].min(),\n",
    "                1.1 * scannet_data[\"near_fars\"].max(),\n",
    "            ]\n",
    "# plt.figure()\n",
    "\n",
    "#subplot(r,c) provide the no. of rows and columns\n",
    "f, axarr = plt.subplots(1,scannet_data[\"depths_h\"].shape[0]) \n",
    "\n",
    "# use the created array to output your multiple images. In this case I have stacked 4 images vertically\n",
    "for i in range(scannet_data[\"depths_h\"].shape[0]):\n",
    "    novel_view_depth_color = visualize_depth(scannet_data[\"depths_h\"][0], depth_minmax)[0]\n",
    "    depth_vis = novel_view_depth_color.permute(1,2,0).numpy()\n",
    "    axarr[i].imshow(depth_vis)\n",
    "\n",
    "\n",
    "# plt.imshow(depth_vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kornia\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from utils.utils import visualize_depth\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "depth = Image.open(\"/home/timothy/Desktop/2023Spring/generalized_nerf/logs_klevr/klevr/0531_architecture_memory_corrected_5/evaluation_/00000000/target_depth_00.png\")\n",
    "# depth = plt.imread(\"/home/timothy/Desktop/2023Spring/generalized_nerf/logs_klevr/klevr/0531_architecture_memory_corrected_5/evaluation_/00000000/target_depth_00.png\")\n",
    "depth = torch.from_numpy(np.array(depth)).to(torch.uint8).to(torch.float32).permute(2,0,1)\n",
    "print(np.asarray(depth_)[0,0])\n",
    "print(depth[:,0,0])\n",
    "\n",
    "print(depth.max())\n",
    "print(depth.shape)\n",
    "print(depth.dtype)\n",
    "filtered_depth = kornia.filters.bilateral_blur(depth.unsqueeze(0), (5, 5), 1, (2, 2))\n",
    "print(filtered_depth.max())\n",
    "print(filtered_depth.shape)\n",
    "print(filtered_depth.dtype)\n",
    "# depth = visualize_depth(depth, [0.1, 17.])[0]\n",
    "# filtered_depth = visualize_depth(filtered_depth, [0.1, 17.])[0]\n",
    "depth = (depth.permute(1,2,0)*255).cpu().numpy().astype(np.uint8)\n",
    "filtered_depth = (filtered_depth.squeeze(0).permute(1,2,0)*255).cpu().numpy().astype(np.uint8)\n",
    "print(depth.shape)\n",
    "print(filtered_depth.shape)\n",
    "print(depth.max())\n",
    "print(filtered_depth.max())\n",
    "f, axarr = plt.subplots(1,2)\n",
    "axarr[0].imshow(depth)\n",
    "axarr[1].imshow(filtered_depth)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1,2,3,4,5,6,7,8,9,10])\n",
    "print(x.shape)\n",
    "print(x.unsqueeze(0).unsqueeze(0).shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0bbd8b2400cb2f722383fac4897cde6fdb10a3a63e2c81520acb32adb5a87cfb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
